{
  "version": 4,
  "terraform_version": "0.12.26",
  "serial": 40,
  "lineage": "403306a7-8b75-8057-6ecb-fb3f75142007",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "google_compute_disk",
      "name": "mdt",
      "each": "list",
      "provider": "provider.google",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "creation_timestamp": "2020-07-02T16:16:05.739-07:00",
            "description": "",
            "disk_encryption_key": [],
            "id": "projects/fluid-faha081j/zones/us-west1-a/disks/lustre-mdt0",
            "image": "",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "last_attach_timestamp": "",
            "last_detach_timestamp": "",
            "name": "lustre-mdt0",
            "physical_block_size_bytes": 4096,
            "project": "fluid-faha081j",
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-mdt0",
            "size": 100,
            "snapshot": "",
            "source_image_encryption_key": [],
            "source_image_id": "",
            "source_snapshot_encryption_key": [],
            "source_snapshot_id": "",
            "timeouts": null,
            "type": "pd-ssd",
            "users": [],
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH19"
        }
      ]
    },
    {
      "mode": "managed",
      "type": "google_compute_disk",
      "name": "ost",
      "each": "list",
      "provider": "provider.google",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "creation_timestamp": "2020-07-02T16:16:05.687-07:00",
            "description": "",
            "disk_encryption_key": [],
            "id": "projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost0",
            "image": "",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "last_attach_timestamp": "",
            "last_detach_timestamp": "",
            "name": "lustre-ost0",
            "physical_block_size_bytes": 4096,
            "project": "fluid-faha081j",
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost0",
            "size": 1000,
            "snapshot": "",
            "source_image_encryption_key": [],
            "source_image_id": "",
            "source_snapshot_encryption_key": [],
            "source_snapshot_id": "",
            "timeouts": null,
            "type": "pd-standard",
            "users": [],
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH19"
        },
        {
          "index_key": 1,
          "schema_version": 0,
          "attributes": {
            "creation_timestamp": "2020-07-02T16:16:05.713-07:00",
            "description": "",
            "disk_encryption_key": [],
            "id": "projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost1",
            "image": "",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "last_attach_timestamp": "",
            "last_detach_timestamp": "",
            "name": "lustre-ost1",
            "physical_block_size_bytes": 4096,
            "project": "fluid-faha081j",
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost1",
            "size": 1000,
            "snapshot": "",
            "source_image_encryption_key": [],
            "source_image_id": "",
            "source_snapshot_encryption_key": [],
            "source_snapshot_id": "",
            "timeouts": null,
            "type": "pd-standard",
            "users": [],
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH19"
        },
        {
          "index_key": 2,
          "schema_version": 0,
          "attributes": {
            "creation_timestamp": "2020-07-02T16:16:05.713-07:00",
            "description": "",
            "disk_encryption_key": [],
            "id": "projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost2",
            "image": "",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "last_attach_timestamp": "",
            "last_detach_timestamp": "",
            "name": "lustre-ost2",
            "physical_block_size_bytes": 4096,
            "project": "fluid-faha081j",
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost2",
            "size": 1000,
            "snapshot": "",
            "source_image_encryption_key": [],
            "source_image_id": "",
            "source_snapshot_encryption_key": [],
            "source_snapshot_id": "",
            "timeouts": null,
            "type": "pd-standard",
            "users": [],
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH19"
        },
        {
          "index_key": 3,
          "schema_version": 0,
          "attributes": {
            "creation_timestamp": "2020-07-02T16:16:05.699-07:00",
            "description": "",
            "disk_encryption_key": [],
            "id": "projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost3",
            "image": "",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "last_attach_timestamp": "",
            "last_detach_timestamp": "",
            "name": "lustre-ost3",
            "physical_block_size_bytes": 4096,
            "project": "fluid-faha081j",
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost3",
            "size": 1000,
            "snapshot": "",
            "source_image_encryption_key": [],
            "source_image_id": "",
            "source_snapshot_encryption_key": [],
            "source_snapshot_id": "",
            "timeouts": null,
            "type": "pd-standard",
            "users": [],
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MjQwMDAwMDAwMDAwLCJ1cGRhdGUiOjI0MDAwMDAwMDAwMH19"
        }
      ]
    },
    {
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "hsm",
      "each": "list",
      "provider": "provider.google",
      "instances": []
    },
    {
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "mds",
      "each": "list",
      "provider": "provider.google",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": true,
            "attached_disk": [
              {
                "device_name": "mdt",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-mdt0"
              }
            ],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200618",
                    "labels": {},
                    "size": 20,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-mds0"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "deletion_protection": false,
            "description": "",
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/fluid-faha081j/zones/us-west1-a/instances/lustre-mds0",
            "instance_id": "5386740438538638422",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-8",
            "metadata": {
              "cluster-name": "lustre",
              "e2fs-version": "latest",
              "enable-oslogin": "TRUE",
              "fs-name": "lustre",
              "hsm-gcs": "",
              "hsm-gcs-prefix": "",
              "lustre-version": "latest-release",
              "node-role": "MDS"
            },
            "metadata_fingerprint": "rXW9ZJh9gJk=",
            "metadata_startup_script": "#!/bin/bash\n\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Imported variables from lustre.jinja, do not modify\nCLUSTER_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/cluster-name\" -H \"Metadata-Flavor: Google\")\nFS_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/fs-name\" -H \"Metadata-Flavor: Google\")\nNODE_ROLE=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/node-role\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET_IMPORT=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs-prefix\" -H \"Metadata-Flavor: Google\")\nLUSTRE_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/lustre-version\" -H \"Metadata-Flavor: Google\")\nE2FS_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/e2fs-version\" -H \"Metadata-Flavor: Google\")\n\nost_mount_point=\"/mnt/ost\"\nmdt_mount_point=\"/mnt/mdt\"\n\nMDS_HOSTNAME=\"${CLUSTER_NAME}-mds0\"\nLUSTRE_CLIENT_VERSION=\"lustre-2.10.8\"\nLUSTRE_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_VERSION}/el7/server/RPMS/x86_64/\"\nLUSTRE_CLIENT_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_CLIENT_VERSION}/el7/client/RPMS/x86_64/\"\nE2FS_URL=\"https://downloads.whamcloud.com/public/e2fsprogs/${E2FS_VERSION}/el7/RPMS/x86_64/\"\n\n# Array of all required RPMs for Lustre\nLUSTRE_RPMS=(\"kernel-3*.rpm\"\n\"kernel-devel-*.rpm\" \n\"kernel-debuginfo-common-*.rpm\"\n\"lustre-2*.rpm\"\n\"lustre-ldiskfs-dkms-*.rpm\"\n\"lustre-osd-ldiskfs-mount-*.rpm\"\n\"kmod-lustre-2*.rpm\"\n\"kmod-lustre-osd-ldiskfs-*.rpm\")\n\nLUSTRE_CLIENT_RPMS=(\"kmod-lustre-client-2*.rpm\"\n\"lustre-client-2*.rpm\")\n\n# Array of all required RPMs for E2FS\nE2FS_RPMS=(\"e2fsprogs-1*.rpm\"\n\"e2fsprogs-libs-1*.rpm\"\n\"libss-1*.rpm\"\n\"libcom_err-1*.rpm\")\n\n# Install updates and minimum packages to install Lustre\nfunction yum_install() {\n\tyum update -y\n\tyum install -y net-snmp-libs expect patch dkms gcc libyaml-devel mdadm epel-release pdsh\n}\n\n# Set Message of the Day declaring that Lustre is being installed\nfunction start_motd() {\n\tmotd=\"*** Lustre is currently being installed in the background. ***\n***  Please wait until notified the installation is done.  ***\"\n\techo \"$motd\" \u003e /etc/motd\n}\n\n#Set Message of the Day to show Lustre cluster information and declare the Lustre installation complete\nfunction end_motd() {\n\techo -e \"Welcome to the Google Cloud Lustre Deployment!\\nLustre MDS: $MDS_HOSTNAME\\nLustre FS Name: $FS_NAME\\nMount Command: mount -t lustre $MDS_HOSTNAME:/$FS_NAME \u003clocal dir\u003e\" \u003e /etc/motd\n\twall -n \"*** Lustre installation is complete! *** \"\n\twall -n \"`cat /etc/motd`\"\n}\n\n# Wait in a loop until the internet is up\nfunction wait_for_internet() {\n\ttime=0\n\t#Loop over one ping to google.com, and while no response\n\twhile [ `ping google.com -c1 | grep \"time=\" -c` -eq 0 ]; do\n\t\tsleep 5\n\t\tlet time+=1\n\t\t# If we try 60 times (300 seconds) without success, abort\n\t\tif [ $time -gt 60 ]; then\n\t\t\techo \"Internet has not connected. Aborting installation.\"\n\t\t\texit 1\n\t\tfi\n\tdone\n}\n\n# Delete RPMs after installation\nfunction cleanup() {\n\trm -rf /lustre/*.rpm\n}\n\n# Install Lemur Lustre HSM Data Mover\nfunction install_lemur() {\n\tyum install -y wget go make rpm-build libselinux-devel libyaml-devel zlib-devel\n\tyum groupinstall -y \"Development Tools\" --skip-broken\n\n\tmkdir /lustre\n\tcd /lustre\n\n\tgit clone https://github.com/mhugues/lemur.git\n\tcd lemur\n\n\tgit checkout feature/lhsm-plugin-gcs\n\n\tsudo make local-rpm\n\n\trpm -ivh /root/rpmbuild/RPMS/x86_64/*.rpm\n\n}\n\nfunction configure_lemur() {\n\t#Lemur Agent Configuration\n\tmkdir -p /var/lib/lhsmd/roots\n\n\tcat \u003e /etc/lhsmd/agent \u003c\u003c EOF\n## The mount target for the Lustre file system that will be used with this agent.\n##\nclient_device=  \"${MDS_HOSTNAME}@tcp:/lustre\"\n\n##\n## Base directory used for the Lustre mount points created by the agent\n##\nmount_root= \"/var/lib/lhsmd/roots\"\n\n##\n## List of enabled plugins\n##\nenabled_plugins = [\"lhsm-plugin-gcs\"]\n\n##\n## Directory to look for the plugins\n##\nplugin_dir = \"/usr/libexec/lhsmd\"\n\n##\n## Number of threads handling incoming HSM requests.\n##\nhandler_count = 8\n\n##\n## Enable experimental snapshot feature.\n##\nsnapshots {\n     enabled = false\n}\n\nEOF\n\n\t#Lemur GCS Plugin Conf\n\tcat \u003e /etc/lhsmd/lhsm-plugin-gcs \u003c\u003c EOF\n## Credentials file in Json format from the service account (Optional)\n#service_account_key=\"[SA_NAME-key.json]\"\n\n## Maximum number of concurrent copies.\n##\nnum_threads=8\n\n##\n## One or more archive definition is required.\n##\narchive \"1\" {\n        id = 1\n        bucket = \"${HSM_GCS_BUCKET:5}\"\n}\nEOF\n\n\tchmod 600 /etc/lhsmd/lhsm-plugin-gcs\n\n\t#Start lhsm agent daemon\n\tsystemctl start lhsmd\n\n\n}\n\nfunction hsm_import_bucket()\n{\n\n\tbucket_file_list=`gsutil ls -r ${HSM_GCS_BUCKET_IMPORT}/** | sed \"/\\/:$/d\"`\n\n\tfor i in $bucket_file_list\n\tdo\n\t\t# Convert to destination file full path\n\t\tdest_file_name=`echo ${i} | sed \"s%${HSM_GCS_BUCKET_IMPORT}%/mnt/%g\"`\n\t\tdir_name=`dirname ${dest_file_name}`\n\n\t\tif [ ! -d ${dir_name} ]; then\n\t\t\tmkdir -p ${dir_name}\n\t\tfi\n\n\t\tsrc_file_name=`echo $i | sed \"s%gs://.[^/]*/%%g\"`\n\t\tlhsm import --uuid ${src_file_name} --uid 0 --gid 0 ${dest_file_name}\n\n\tdone\n}\n\nfunction install_lustre_client() {\n\n\tyum install -y git\n\n\tgit clone git://git.whamcloud.com/fs/lustre-release.git \u0026\u0026 cd lustre-release\n\tgit checkout 2.10.8\n\n\tyum install -y kernel-devel kernel-headers kernel-debug libtool libyaml-devel libselinux-devel zlib-devel rpm-build\n\n\tsh ./autogen.sh\n\t./configure --disable-server --with-o2ib=no --enable-client\n\tmake rpms\n\n\tmkdir -p /lustre/lustre_client_release\n\tcp *.rpm /lustre/lustre_client_release/\n\trpm -ivh /lustre/lustre_client_release/${LUSTRE_CLIENT_RPMS[@]}\n\n}\n\nfunction main() {\n\t# Check for an install.log, if it doesn't exist begin install\n\tif [ ! -e /lustre/install.log ]; then\n\t\tstart_motd\n\t\twait_for_internet\n\t\t# Install wget\n\t\tyum install -y wget\n\n\t\tmkdir /lustre\n\t\tcd /lustre\n\n\t\tif [ \"$NODE_ROLE\" != \"HSM\" ]; then\n\t\t\t# Update and install packages in background while RPMs are downloaded\n\t\t\tyum_install \u0026\n\n\t\t\t# Loop over RPM arrays and download them locally\n\t\t\tfor i in ${LUSTRE_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${LUSTRE_URL} -P /lustre; done\n\t\t\tfor i in ${E2FS_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${E2FS_URL} -P /lustre; done\n\t\t\tfind /lustre -name \"*.rpm\" | xargs -I{} mv {} /lustre\n\t\t\n\t\t\t# Wait for yum_install for finish\n\t\t\twait `jobs -p`\n\t\tfi\n\t\t\n\t\t# Disable yum-crom, firewalld, and disable SELINUX (Lustre requirements)\n\t\tsystemctl disable yum-cron.service\n\t\tsystemctl stop yum-cron\n\t\tsed -i -e \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config\n\t\tsystemctl stop firewalld\n\t\tsystemctl disable firewalld\n\t\t\n\t\t# Install all downloaded RPMs\n\t\trpm -ivh --force *.rpm\n\t\tif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\tinstall_lustre_client\n\t\tfi\n\t\n\t\t# Mark install.log as reaching stage 1\n\t\techo 1 \u003e /lustre/install.log\n\t\t# Reboot to switch to the Lustre kernel\n\t\treboot\n\t# If the install.log exists, and contains a 1, begin installation stage 2\n\telif [ \"`cat /lustre/install.log`\" == \"1\" ]; then\n\t\t# Load the Lustre kernel modules\n\t\tmodprobe lustre\n\t\t\n\t\t# Get the hostname index (trailing digit on the hostname) \n\t\thost_index=`hostname | grep -o -e '[[:digit:]]*' | tail -1`\n\t\t# Decrement the index by 1 to convert to Lustre's indexing\n\t\tif [ ! $host_index ]; then\n\t\t\thost_index=0\n\t\telse\n\t\t\tlet host_index=host_index-1\n\t\tfi\n\n\t\t# Determine if the OST/MDT disk is PD or Local SSD\n        \tnum_local_ssds=`lsblk | grep -c nvme`\n        \tif [ ${num_local_ssds} -gt 1 ]; then\n\t        \tlustre_device=\"/dev/md0\"\n\t        \tsudo mdadm --create ${lustre_device} --level=0 --raid-devices=${num_local_ssds} /dev/nvme0n*\n        \telif [ $num_local_ssds -eq 1 ]; then\n\t        \tlustre_device=\"/dev/nvme0n1\"\n        \telse\n\t        \tlustre_device=\"/dev/sdb\"\n        \tfi\n\t\t\n\t\t# If the local node running this script is a Lustre MDS, install the MDS/MGS software\n\t\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response\n\t\t\twhile [ `sudo lctl ping ${CLUSTER_NAME}-oss0 | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\tmkdir $mdt_mount_point\n\t\t\tmkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$mdt_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\n\t\t\t# Enable HSM on the Lustre MGS\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm_control=enabled\n\n\t\t\t# Setup the archive id for the specific HSM backend, we are only using 1 so id=1 is just fine\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm.default_archive_id=1\n\n\t\t\t# Increase the number of HSM Max requests on the MDT, you may want to\n\t\t\t# experiment with various values if you intend to go to production\n\t\t\tlctl set_param mdt.*-MDT0000.hsm.max_requests=128\n\n\t\t\t# Once the network is up, make the lustre filesystem on the MDT\n\t\t\t#mkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} /dev/sdb\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\t#mkdir /mdt\n\t\t\t#mount -t lustre /dev/sdb /mdt\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $mdt_mount_point` -eq 0 ]; then\n\t\t\t\techo \"MDT mount has failed. Please try mounting manually with \"mount -t lustre $lustre_device $mdt_mount_point\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\n\t\t\t# Disable the authentication upcall by default, change if using auth\n\t\t\techo NONE \u003e /proc/fs/lustre/mdt/lustre-MDT0000/identity_upcall\n\t\t# If the local node running this script is a Lustre OSS, install the OSS software\n\t\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the Lustre OST\n\t\t\t# Sleep 60 seconds to give MDS/MGS time to come up before the OSS. More robust communication would be good.\n\t\t\tsleep 60\n\n\t\t\t# Make the directory to mount the OST, and mount the OST\n\t\t\tmkdir $ost_mount_point\n\t\t\tmkfs.lustre --ost --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$ost_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $ost_mount_point` -eq 0 ]; then\n\t\t\t\techo \"OST mount has failed. Please try mounting manually with \\\"mount -t lustre $lustre_device $ost_mount_point\\\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\t\t# If the local node running this script is a Lustre HSM Data Mover, install the Lemur software\n\t\telif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\tmount_status=1\n\n\t\t\twhile [ ${mount_status} -ne 0 ]; do\n\t\t\t\tmount -t lustre ${MDS_HOSTNAME}:/${FS_NAME} /mnt\n\t\t\t\tmount_status=$?\n\t\t\tdone\n\n\t\t\tinstall_lemur\n\t\t\tconfigure_lemur\n\n\t\t\tif [ ! -z \"${HSM_GCS_BUCKET_IMPORT}\" ]; then\n\t\t\t\thsm_import_bucket\n\t\t\tfi\n\n\t\tfi\n\t\t# Mark install.log as reaching stage 2\n\t\techo 2 \u003e /lustre/install.log\n\t\t# Change MOTD to mark install as complete\n\t\tend_motd\n\t\t# Run the cleanup function to remove RPMs\n\t\tcleanup\n\t# If install.log shows stage 2, then Lustre is installed and just needs to be started\n\t#else\n\t\t# If it's an MDS/MGS, mount the MDT\n\t#\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t#\t\tmount -t lustre /dev/sdb /mdt\n\t\t# If it's an OSS, sleep to let the MDT mount, and then mount the OST\n\t#\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t#\t\tsleep 20\n\t#\t\tmount -t lustre /dev/sdb $ost_mount_point\n\t#\tfi\n\tfi\n}\n\nmain $@\n",
            "min_cpu_platform": "",
            "name": "lustre-mds0",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "34.105.121.152",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/global/networks/default",
                "network_ip": "10.138.0.33",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/regions/us-west1/subnetworks/default",
                "subnetwork_project": "fluid-faha081j"
              }
            ],
            "project": "fluid-faha081j",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/instances/lustre-mds0",
            "service_account": [
              {
                "email": "201229081417-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "lustre"
            ],
            "tags_fingerprint": "CgvKniLKZUs=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "google_compute_disk.mdt"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "google_compute_instance",
      "name": "oss",
      "each": "list",
      "provider": "provider.google",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": true,
            "attached_disk": [
              {
                "device_name": "ost",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost0"
              }
            ],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200618",
                    "labels": {},
                    "size": 20,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-oss0"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "deletion_protection": false,
            "description": "",
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss0",
            "instance_id": "670766923332703318",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-8",
            "metadata": {
              "cluster-name": "lustre",
              "e2fs-version": "latest",
              "enable-oslogin": "TRUE",
              "fs-name": "lustre",
              "hsm-gcs": "",
              "hsm-gcs-prefix": "",
              "lustre-version": "latest-release",
              "node-role": "OSS"
            },
            "metadata_fingerprint": "8Buto1H12no=",
            "metadata_startup_script": "#!/bin/bash\n\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Imported variables from lustre.jinja, do not modify\nCLUSTER_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/cluster-name\" -H \"Metadata-Flavor: Google\")\nFS_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/fs-name\" -H \"Metadata-Flavor: Google\")\nNODE_ROLE=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/node-role\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET_IMPORT=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs-prefix\" -H \"Metadata-Flavor: Google\")\nLUSTRE_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/lustre-version\" -H \"Metadata-Flavor: Google\")\nE2FS_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/e2fs-version\" -H \"Metadata-Flavor: Google\")\n\nost_mount_point=\"/mnt/ost\"\nmdt_mount_point=\"/mnt/mdt\"\n\nMDS_HOSTNAME=\"${CLUSTER_NAME}-mds0\"\nLUSTRE_CLIENT_VERSION=\"lustre-2.10.8\"\nLUSTRE_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_VERSION}/el7/server/RPMS/x86_64/\"\nLUSTRE_CLIENT_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_CLIENT_VERSION}/el7/client/RPMS/x86_64/\"\nE2FS_URL=\"https://downloads.whamcloud.com/public/e2fsprogs/${E2FS_VERSION}/el7/RPMS/x86_64/\"\n\n# Array of all required RPMs for Lustre\nLUSTRE_RPMS=(\"kernel-3*.rpm\"\n\"kernel-devel-*.rpm\" \n\"kernel-debuginfo-common-*.rpm\"\n\"lustre-2*.rpm\"\n\"lustre-ldiskfs-dkms-*.rpm\"\n\"lustre-osd-ldiskfs-mount-*.rpm\"\n\"kmod-lustre-2*.rpm\"\n\"kmod-lustre-osd-ldiskfs-*.rpm\")\n\nLUSTRE_CLIENT_RPMS=(\"kmod-lustre-client-2*.rpm\"\n\"lustre-client-2*.rpm\")\n\n# Array of all required RPMs for E2FS\nE2FS_RPMS=(\"e2fsprogs-1*.rpm\"\n\"e2fsprogs-libs-1*.rpm\"\n\"libss-1*.rpm\"\n\"libcom_err-1*.rpm\")\n\n# Install updates and minimum packages to install Lustre\nfunction yum_install() {\n\tyum update -y\n\tyum install -y net-snmp-libs expect patch dkms gcc libyaml-devel mdadm epel-release pdsh\n}\n\n# Set Message of the Day declaring that Lustre is being installed\nfunction start_motd() {\n\tmotd=\"*** Lustre is currently being installed in the background. ***\n***  Please wait until notified the installation is done.  ***\"\n\techo \"$motd\" \u003e /etc/motd\n}\n\n#Set Message of the Day to show Lustre cluster information and declare the Lustre installation complete\nfunction end_motd() {\n\techo -e \"Welcome to the Google Cloud Lustre Deployment!\\nLustre MDS: $MDS_HOSTNAME\\nLustre FS Name: $FS_NAME\\nMount Command: mount -t lustre $MDS_HOSTNAME:/$FS_NAME \u003clocal dir\u003e\" \u003e /etc/motd\n\twall -n \"*** Lustre installation is complete! *** \"\n\twall -n \"`cat /etc/motd`\"\n}\n\n# Wait in a loop until the internet is up\nfunction wait_for_internet() {\n\ttime=0\n\t#Loop over one ping to google.com, and while no response\n\twhile [ `ping google.com -c1 | grep \"time=\" -c` -eq 0 ]; do\n\t\tsleep 5\n\t\tlet time+=1\n\t\t# If we try 60 times (300 seconds) without success, abort\n\t\tif [ $time -gt 60 ]; then\n\t\t\techo \"Internet has not connected. Aborting installation.\"\n\t\t\texit 1\n\t\tfi\n\tdone\n}\n\n# Delete RPMs after installation\nfunction cleanup() {\n\trm -rf /lustre/*.rpm\n}\n\n# Install Lemur Lustre HSM Data Mover\nfunction install_lemur() {\n\tyum install -y wget go make rpm-build libselinux-devel libyaml-devel zlib-devel\n\tyum groupinstall -y \"Development Tools\" --skip-broken\n\n\tmkdir /lustre\n\tcd /lustre\n\n\tgit clone https://github.com/mhugues/lemur.git\n\tcd lemur\n\n\tgit checkout feature/lhsm-plugin-gcs\n\n\tsudo make local-rpm\n\n\trpm -ivh /root/rpmbuild/RPMS/x86_64/*.rpm\n\n}\n\nfunction configure_lemur() {\n\t#Lemur Agent Configuration\n\tmkdir -p /var/lib/lhsmd/roots\n\n\tcat \u003e /etc/lhsmd/agent \u003c\u003c EOF\n## The mount target for the Lustre file system that will be used with this agent.\n##\nclient_device=  \"${MDS_HOSTNAME}@tcp:/lustre\"\n\n##\n## Base directory used for the Lustre mount points created by the agent\n##\nmount_root= \"/var/lib/lhsmd/roots\"\n\n##\n## List of enabled plugins\n##\nenabled_plugins = [\"lhsm-plugin-gcs\"]\n\n##\n## Directory to look for the plugins\n##\nplugin_dir = \"/usr/libexec/lhsmd\"\n\n##\n## Number of threads handling incoming HSM requests.\n##\nhandler_count = 8\n\n##\n## Enable experimental snapshot feature.\n##\nsnapshots {\n     enabled = false\n}\n\nEOF\n\n\t#Lemur GCS Plugin Conf\n\tcat \u003e /etc/lhsmd/lhsm-plugin-gcs \u003c\u003c EOF\n## Credentials file in Json format from the service account (Optional)\n#service_account_key=\"[SA_NAME-key.json]\"\n\n## Maximum number of concurrent copies.\n##\nnum_threads=8\n\n##\n## One or more archive definition is required.\n##\narchive \"1\" {\n        id = 1\n        bucket = \"${HSM_GCS_BUCKET:5}\"\n}\nEOF\n\n\tchmod 600 /etc/lhsmd/lhsm-plugin-gcs\n\n\t#Start lhsm agent daemon\n\tsystemctl start lhsmd\n\n\n}\n\nfunction hsm_import_bucket()\n{\n\n\tbucket_file_list=`gsutil ls -r ${HSM_GCS_BUCKET_IMPORT}/** | sed \"/\\/:$/d\"`\n\n\tfor i in $bucket_file_list\n\tdo\n\t\t# Convert to destination file full path\n\t\tdest_file_name=`echo ${i} | sed \"s%${HSM_GCS_BUCKET_IMPORT}%/mnt/%g\"`\n\t\tdir_name=`dirname ${dest_file_name}`\n\n\t\tif [ ! -d ${dir_name} ]; then\n\t\t\tmkdir -p ${dir_name}\n\t\tfi\n\n\t\tsrc_file_name=`echo $i | sed \"s%gs://.[^/]*/%%g\"`\n\t\tlhsm import --uuid ${src_file_name} --uid 0 --gid 0 ${dest_file_name}\n\n\tdone\n}\n\nfunction install_lustre_client() {\n\n\tyum install -y git\n\n\tgit clone git://git.whamcloud.com/fs/lustre-release.git \u0026\u0026 cd lustre-release\n\tgit checkout 2.10.8\n\n\tyum install -y kernel-devel kernel-headers kernel-debug libtool libyaml-devel libselinux-devel zlib-devel rpm-build\n\n\tsh ./autogen.sh\n\t./configure --disable-server --with-o2ib=no --enable-client\n\tmake rpms\n\n\tmkdir -p /lustre/lustre_client_release\n\tcp *.rpm /lustre/lustre_client_release/\n\trpm -ivh /lustre/lustre_client_release/${LUSTRE_CLIENT_RPMS[@]}\n\n}\n\nfunction main() {\n\t# Check for an install.log, if it doesn't exist begin install\n\tif [ ! -e /lustre/install.log ]; then\n\t\tstart_motd\n\t\twait_for_internet\n\t\t# Install wget\n\t\tyum install -y wget\n\n\t\tmkdir /lustre\n\t\tcd /lustre\n\n\t\tif [ \"$NODE_ROLE\" != \"HSM\" ]; then\n\t\t\t# Update and install packages in background while RPMs are downloaded\n\t\t\tyum_install \u0026\n\n\t\t\t# Loop over RPM arrays and download them locally\n\t\t\tfor i in ${LUSTRE_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${LUSTRE_URL} -P /lustre; done\n\t\t\tfor i in ${E2FS_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${E2FS_URL} -P /lustre; done\n\t\t\tfind /lustre -name \"*.rpm\" | xargs -I{} mv {} /lustre\n\t\t\n\t\t\t# Wait for yum_install for finish\n\t\t\twait `jobs -p`\n\t\tfi\n\t\t\n\t\t# Disable yum-crom, firewalld, and disable SELINUX (Lustre requirements)\n\t\tsystemctl disable yum-cron.service\n\t\tsystemctl stop yum-cron\n\t\tsed -i -e \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config\n\t\tsystemctl stop firewalld\n\t\tsystemctl disable firewalld\n\t\t\n\t\t# Install all downloaded RPMs\n\t\trpm -ivh --force *.rpm\n\t\tif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\tinstall_lustre_client\n\t\tfi\n\t\n\t\t# Mark install.log as reaching stage 1\n\t\techo 1 \u003e /lustre/install.log\n\t\t# Reboot to switch to the Lustre kernel\n\t\treboot\n\t# If the install.log exists, and contains a 1, begin installation stage 2\n\telif [ \"`cat /lustre/install.log`\" == \"1\" ]; then\n\t\t# Load the Lustre kernel modules\n\t\tmodprobe lustre\n\t\t\n\t\t# Get the hostname index (trailing digit on the hostname) \n\t\thost_index=`hostname | grep -o -e '[[:digit:]]*' | tail -1`\n\t\t# Decrement the index by 1 to convert to Lustre's indexing\n\t\tif [ ! $host_index ]; then\n\t\t\thost_index=0\n\t\telse\n\t\t\tlet host_index=host_index-1\n\t\tfi\n\n\t\t# Determine if the OST/MDT disk is PD or Local SSD\n        \tnum_local_ssds=`lsblk | grep -c nvme`\n        \tif [ ${num_local_ssds} -gt 1 ]; then\n\t        \tlustre_device=\"/dev/md0\"\n\t        \tsudo mdadm --create ${lustre_device} --level=0 --raid-devices=${num_local_ssds} /dev/nvme0n*\n        \telif [ $num_local_ssds -eq 1 ]; then\n\t        \tlustre_device=\"/dev/nvme0n1\"\n        \telse\n\t        \tlustre_device=\"/dev/sdb\"\n        \tfi\n\t\t\n\t\t# If the local node running this script is a Lustre MDS, install the MDS/MGS software\n\t\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response\n\t\t\twhile [ `sudo lctl ping ${CLUSTER_NAME}-oss0 | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\tmkdir $mdt_mount_point\n\t\t\tmkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$mdt_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\n\t\t\t# Enable HSM on the Lustre MGS\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm_control=enabled\n\n\t\t\t# Setup the archive id for the specific HSM backend, we are only using 1 so id=1 is just fine\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm.default_archive_id=1\n\n\t\t\t# Increase the number of HSM Max requests on the MDT, you may want to\n\t\t\t# experiment with various values if you intend to go to production\n\t\t\tlctl set_param mdt.*-MDT0000.hsm.max_requests=128\n\n\t\t\t# Once the network is up, make the lustre filesystem on the MDT\n\t\t\t#mkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} /dev/sdb\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\t#mkdir /mdt\n\t\t\t#mount -t lustre /dev/sdb /mdt\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $mdt_mount_point` -eq 0 ]; then\n\t\t\t\techo \"MDT mount has failed. Please try mounting manually with \"mount -t lustre $lustre_device $mdt_mount_point\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\n\t\t\t# Disable the authentication upcall by default, change if using auth\n\t\t\techo NONE \u003e /proc/fs/lustre/mdt/lustre-MDT0000/identity_upcall\n\t\t# If the local node running this script is a Lustre OSS, install the OSS software\n\t\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the Lustre OST\n\t\t\t# Sleep 60 seconds to give MDS/MGS time to come up before the OSS. More robust communication would be good.\n\t\t\tsleep 60\n\n\t\t\t# Make the directory to mount the OST, and mount the OST\n\t\t\tmkdir $ost_mount_point\n\t\t\tmkfs.lustre --ost --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$ost_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $ost_mount_point` -eq 0 ]; then\n\t\t\t\techo \"OST mount has failed. Please try mounting manually with \\\"mount -t lustre $lustre_device $ost_mount_point\\\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\t\t# If the local node running this script is a Lustre HSM Data Mover, install the Lemur software\n\t\telif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\tmount_status=1\n\n\t\t\twhile [ ${mount_status} -ne 0 ]; do\n\t\t\t\tmount -t lustre ${MDS_HOSTNAME}:/${FS_NAME} /mnt\n\t\t\t\tmount_status=$?\n\t\t\tdone\n\n\t\t\tinstall_lemur\n\t\t\tconfigure_lemur\n\n\t\t\tif [ ! -z \"${HSM_GCS_BUCKET_IMPORT}\" ]; then\n\t\t\t\thsm_import_bucket\n\t\t\tfi\n\n\t\tfi\n\t\t# Mark install.log as reaching stage 2\n\t\techo 2 \u003e /lustre/install.log\n\t\t# Change MOTD to mark install as complete\n\t\tend_motd\n\t\t# Run the cleanup function to remove RPMs\n\t\tcleanup\n\t# If install.log shows stage 2, then Lustre is installed and just needs to be started\n\t#else\n\t\t# If it's an MDS/MGS, mount the MDT\n\t#\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t#\t\tmount -t lustre /dev/sdb /mdt\n\t\t# If it's an OSS, sleep to let the MDT mount, and then mount the OST\n\t#\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t#\t\tsleep 20\n\t#\t\tmount -t lustre /dev/sdb $ost_mount_point\n\t#\tfi\n\tfi\n}\n\nmain $@\n",
            "min_cpu_platform": "",
            "name": "lustre-oss0",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "35.197.39.76",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/global/networks/default",
                "network_ip": "10.138.0.34",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/regions/us-west1/subnetworks/default",
                "subnetwork_project": "fluid-faha081j"
              }
            ],
            "project": "fluid-faha081j",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss0",
            "service_account": [
              {
                "email": "201229081417-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "lustre"
            ],
            "tags_fingerprint": "CgvKniLKZUs=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "google_compute_disk.ost"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": true,
            "attached_disk": [
              {
                "device_name": "ost",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost1"
              }
            ],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200618",
                    "labels": {},
                    "size": 20,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-oss1"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "deletion_protection": false,
            "description": "",
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss1",
            "instance_id": "4650884722507109462",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-8",
            "metadata": {
              "cluster-name": "lustre",
              "e2fs-version": "latest",
              "enable-oslogin": "TRUE",
              "fs-name": "lustre",
              "hsm-gcs": "",
              "hsm-gcs-prefix": "",
              "lustre-version": "latest-release",
              "node-role": "OSS"
            },
            "metadata_fingerprint": "8Buto1H12no=",
            "metadata_startup_script": "#!/bin/bash\n\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Imported variables from lustre.jinja, do not modify\nCLUSTER_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/cluster-name\" -H \"Metadata-Flavor: Google\")\nFS_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/fs-name\" -H \"Metadata-Flavor: Google\")\nNODE_ROLE=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/node-role\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET_IMPORT=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs-prefix\" -H \"Metadata-Flavor: Google\")\nLUSTRE_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/lustre-version\" -H \"Metadata-Flavor: Google\")\nE2FS_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/e2fs-version\" -H \"Metadata-Flavor: Google\")\n\nost_mount_point=\"/mnt/ost\"\nmdt_mount_point=\"/mnt/mdt\"\n\nMDS_HOSTNAME=\"${CLUSTER_NAME}-mds0\"\nLUSTRE_CLIENT_VERSION=\"lustre-2.10.8\"\nLUSTRE_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_VERSION}/el7/server/RPMS/x86_64/\"\nLUSTRE_CLIENT_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_CLIENT_VERSION}/el7/client/RPMS/x86_64/\"\nE2FS_URL=\"https://downloads.whamcloud.com/public/e2fsprogs/${E2FS_VERSION}/el7/RPMS/x86_64/\"\n\n# Array of all required RPMs for Lustre\nLUSTRE_RPMS=(\"kernel-3*.rpm\"\n\"kernel-devel-*.rpm\" \n\"kernel-debuginfo-common-*.rpm\"\n\"lustre-2*.rpm\"\n\"lustre-ldiskfs-dkms-*.rpm\"\n\"lustre-osd-ldiskfs-mount-*.rpm\"\n\"kmod-lustre-2*.rpm\"\n\"kmod-lustre-osd-ldiskfs-*.rpm\")\n\nLUSTRE_CLIENT_RPMS=(\"kmod-lustre-client-2*.rpm\"\n\"lustre-client-2*.rpm\")\n\n# Array of all required RPMs for E2FS\nE2FS_RPMS=(\"e2fsprogs-1*.rpm\"\n\"e2fsprogs-libs-1*.rpm\"\n\"libss-1*.rpm\"\n\"libcom_err-1*.rpm\")\n\n# Install updates and minimum packages to install Lustre\nfunction yum_install() {\n\tyum update -y\n\tyum install -y net-snmp-libs expect patch dkms gcc libyaml-devel mdadm epel-release pdsh\n}\n\n# Set Message of the Day declaring that Lustre is being installed\nfunction start_motd() {\n\tmotd=\"*** Lustre is currently being installed in the background. ***\n***  Please wait until notified the installation is done.  ***\"\n\techo \"$motd\" \u003e /etc/motd\n}\n\n#Set Message of the Day to show Lustre cluster information and declare the Lustre installation complete\nfunction end_motd() {\n\techo -e \"Welcome to the Google Cloud Lustre Deployment!\\nLustre MDS: $MDS_HOSTNAME\\nLustre FS Name: $FS_NAME\\nMount Command: mount -t lustre $MDS_HOSTNAME:/$FS_NAME \u003clocal dir\u003e\" \u003e /etc/motd\n\twall -n \"*** Lustre installation is complete! *** \"\n\twall -n \"`cat /etc/motd`\"\n}\n\n# Wait in a loop until the internet is up\nfunction wait_for_internet() {\n\ttime=0\n\t#Loop over one ping to google.com, and while no response\n\twhile [ `ping google.com -c1 | grep \"time=\" -c` -eq 0 ]; do\n\t\tsleep 5\n\t\tlet time+=1\n\t\t# If we try 60 times (300 seconds) without success, abort\n\t\tif [ $time -gt 60 ]; then\n\t\t\techo \"Internet has not connected. Aborting installation.\"\n\t\t\texit 1\n\t\tfi\n\tdone\n}\n\n# Delete RPMs after installation\nfunction cleanup() {\n\trm -rf /lustre/*.rpm\n}\n\n# Install Lemur Lustre HSM Data Mover\nfunction install_lemur() {\n\tyum install -y wget go make rpm-build libselinux-devel libyaml-devel zlib-devel\n\tyum groupinstall -y \"Development Tools\" --skip-broken\n\n\tmkdir /lustre\n\tcd /lustre\n\n\tgit clone https://github.com/mhugues/lemur.git\n\tcd lemur\n\n\tgit checkout feature/lhsm-plugin-gcs\n\n\tsudo make local-rpm\n\n\trpm -ivh /root/rpmbuild/RPMS/x86_64/*.rpm\n\n}\n\nfunction configure_lemur() {\n\t#Lemur Agent Configuration\n\tmkdir -p /var/lib/lhsmd/roots\n\n\tcat \u003e /etc/lhsmd/agent \u003c\u003c EOF\n## The mount target for the Lustre file system that will be used with this agent.\n##\nclient_device=  \"${MDS_HOSTNAME}@tcp:/lustre\"\n\n##\n## Base directory used for the Lustre mount points created by the agent\n##\nmount_root= \"/var/lib/lhsmd/roots\"\n\n##\n## List of enabled plugins\n##\nenabled_plugins = [\"lhsm-plugin-gcs\"]\n\n##\n## Directory to look for the plugins\n##\nplugin_dir = \"/usr/libexec/lhsmd\"\n\n##\n## Number of threads handling incoming HSM requests.\n##\nhandler_count = 8\n\n##\n## Enable experimental snapshot feature.\n##\nsnapshots {\n     enabled = false\n}\n\nEOF\n\n\t#Lemur GCS Plugin Conf\n\tcat \u003e /etc/lhsmd/lhsm-plugin-gcs \u003c\u003c EOF\n## Credentials file in Json format from the service account (Optional)\n#service_account_key=\"[SA_NAME-key.json]\"\n\n## Maximum number of concurrent copies.\n##\nnum_threads=8\n\n##\n## One or more archive definition is required.\n##\narchive \"1\" {\n        id = 1\n        bucket = \"${HSM_GCS_BUCKET:5}\"\n}\nEOF\n\n\tchmod 600 /etc/lhsmd/lhsm-plugin-gcs\n\n\t#Start lhsm agent daemon\n\tsystemctl start lhsmd\n\n\n}\n\nfunction hsm_import_bucket()\n{\n\n\tbucket_file_list=`gsutil ls -r ${HSM_GCS_BUCKET_IMPORT}/** | sed \"/\\/:$/d\"`\n\n\tfor i in $bucket_file_list\n\tdo\n\t\t# Convert to destination file full path\n\t\tdest_file_name=`echo ${i} | sed \"s%${HSM_GCS_BUCKET_IMPORT}%/mnt/%g\"`\n\t\tdir_name=`dirname ${dest_file_name}`\n\n\t\tif [ ! -d ${dir_name} ]; then\n\t\t\tmkdir -p ${dir_name}\n\t\tfi\n\n\t\tsrc_file_name=`echo $i | sed \"s%gs://.[^/]*/%%g\"`\n\t\tlhsm import --uuid ${src_file_name} --uid 0 --gid 0 ${dest_file_name}\n\n\tdone\n}\n\nfunction install_lustre_client() {\n\n\tyum install -y git\n\n\tgit clone git://git.whamcloud.com/fs/lustre-release.git \u0026\u0026 cd lustre-release\n\tgit checkout 2.10.8\n\n\tyum install -y kernel-devel kernel-headers kernel-debug libtool libyaml-devel libselinux-devel zlib-devel rpm-build\n\n\tsh ./autogen.sh\n\t./configure --disable-server --with-o2ib=no --enable-client\n\tmake rpms\n\n\tmkdir -p /lustre/lustre_client_release\n\tcp *.rpm /lustre/lustre_client_release/\n\trpm -ivh /lustre/lustre_client_release/${LUSTRE_CLIENT_RPMS[@]}\n\n}\n\nfunction main() {\n\t# Check for an install.log, if it doesn't exist begin install\n\tif [ ! -e /lustre/install.log ]; then\n\t\tstart_motd\n\t\twait_for_internet\n\t\t# Install wget\n\t\tyum install -y wget\n\n\t\tmkdir /lustre\n\t\tcd /lustre\n\n\t\tif [ \"$NODE_ROLE\" != \"HSM\" ]; then\n\t\t\t# Update and install packages in background while RPMs are downloaded\n\t\t\tyum_install \u0026\n\n\t\t\t# Loop over RPM arrays and download them locally\n\t\t\tfor i in ${LUSTRE_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${LUSTRE_URL} -P /lustre; done\n\t\t\tfor i in ${E2FS_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${E2FS_URL} -P /lustre; done\n\t\t\tfind /lustre -name \"*.rpm\" | xargs -I{} mv {} /lustre\n\t\t\n\t\t\t# Wait for yum_install for finish\n\t\t\twait `jobs -p`\n\t\tfi\n\t\t\n\t\t# Disable yum-crom, firewalld, and disable SELINUX (Lustre requirements)\n\t\tsystemctl disable yum-cron.service\n\t\tsystemctl stop yum-cron\n\t\tsed -i -e \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config\n\t\tsystemctl stop firewalld\n\t\tsystemctl disable firewalld\n\t\t\n\t\t# Install all downloaded RPMs\n\t\trpm -ivh --force *.rpm\n\t\tif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\tinstall_lustre_client\n\t\tfi\n\t\n\t\t# Mark install.log as reaching stage 1\n\t\techo 1 \u003e /lustre/install.log\n\t\t# Reboot to switch to the Lustre kernel\n\t\treboot\n\t# If the install.log exists, and contains a 1, begin installation stage 2\n\telif [ \"`cat /lustre/install.log`\" == \"1\" ]; then\n\t\t# Load the Lustre kernel modules\n\t\tmodprobe lustre\n\t\t\n\t\t# Get the hostname index (trailing digit on the hostname) \n\t\thost_index=`hostname | grep -o -e '[[:digit:]]*' | tail -1`\n\t\t# Decrement the index by 1 to convert to Lustre's indexing\n\t\tif [ ! $host_index ]; then\n\t\t\thost_index=0\n\t\telse\n\t\t\tlet host_index=host_index-1\n\t\tfi\n\n\t\t# Determine if the OST/MDT disk is PD or Local SSD\n        \tnum_local_ssds=`lsblk | grep -c nvme`\n        \tif [ ${num_local_ssds} -gt 1 ]; then\n\t        \tlustre_device=\"/dev/md0\"\n\t        \tsudo mdadm --create ${lustre_device} --level=0 --raid-devices=${num_local_ssds} /dev/nvme0n*\n        \telif [ $num_local_ssds -eq 1 ]; then\n\t        \tlustre_device=\"/dev/nvme0n1\"\n        \telse\n\t        \tlustre_device=\"/dev/sdb\"\n        \tfi\n\t\t\n\t\t# If the local node running this script is a Lustre MDS, install the MDS/MGS software\n\t\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response\n\t\t\twhile [ `sudo lctl ping ${CLUSTER_NAME}-oss0 | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\tmkdir $mdt_mount_point\n\t\t\tmkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$mdt_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\n\t\t\t# Enable HSM on the Lustre MGS\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm_control=enabled\n\n\t\t\t# Setup the archive id for the specific HSM backend, we are only using 1 so id=1 is just fine\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm.default_archive_id=1\n\n\t\t\t# Increase the number of HSM Max requests on the MDT, you may want to\n\t\t\t# experiment with various values if you intend to go to production\n\t\t\tlctl set_param mdt.*-MDT0000.hsm.max_requests=128\n\n\t\t\t# Once the network is up, make the lustre filesystem on the MDT\n\t\t\t#mkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} /dev/sdb\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\t#mkdir /mdt\n\t\t\t#mount -t lustre /dev/sdb /mdt\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $mdt_mount_point` -eq 0 ]; then\n\t\t\t\techo \"MDT mount has failed. Please try mounting manually with \"mount -t lustre $lustre_device $mdt_mount_point\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\n\t\t\t# Disable the authentication upcall by default, change if using auth\n\t\t\techo NONE \u003e /proc/fs/lustre/mdt/lustre-MDT0000/identity_upcall\n\t\t# If the local node running this script is a Lustre OSS, install the OSS software\n\t\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the Lustre OST\n\t\t\t# Sleep 60 seconds to give MDS/MGS time to come up before the OSS. More robust communication would be good.\n\t\t\tsleep 60\n\n\t\t\t# Make the directory to mount the OST, and mount the OST\n\t\t\tmkdir $ost_mount_point\n\t\t\tmkfs.lustre --ost --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$ost_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $ost_mount_point` -eq 0 ]; then\n\t\t\t\techo \"OST mount has failed. Please try mounting manually with \\\"mount -t lustre $lustre_device $ost_mount_point\\\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\t\t# If the local node running this script is a Lustre HSM Data Mover, install the Lemur software\n\t\telif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\tmount_status=1\n\n\t\t\twhile [ ${mount_status} -ne 0 ]; do\n\t\t\t\tmount -t lustre ${MDS_HOSTNAME}:/${FS_NAME} /mnt\n\t\t\t\tmount_status=$?\n\t\t\tdone\n\n\t\t\tinstall_lemur\n\t\t\tconfigure_lemur\n\n\t\t\tif [ ! -z \"${HSM_GCS_BUCKET_IMPORT}\" ]; then\n\t\t\t\thsm_import_bucket\n\t\t\tfi\n\n\t\tfi\n\t\t# Mark install.log as reaching stage 2\n\t\techo 2 \u003e /lustre/install.log\n\t\t# Change MOTD to mark install as complete\n\t\tend_motd\n\t\t# Run the cleanup function to remove RPMs\n\t\tcleanup\n\t# If install.log shows stage 2, then Lustre is installed and just needs to be started\n\t#else\n\t\t# If it's an MDS/MGS, mount the MDT\n\t#\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t#\t\tmount -t lustre /dev/sdb /mdt\n\t\t# If it's an OSS, sleep to let the MDT mount, and then mount the OST\n\t#\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t#\t\tsleep 20\n\t#\t\tmount -t lustre /dev/sdb $ost_mount_point\n\t#\tfi\n\tfi\n}\n\nmain $@\n",
            "min_cpu_platform": "",
            "name": "lustre-oss1",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "34.82.164.109",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/global/networks/default",
                "network_ip": "10.138.0.35",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/regions/us-west1/subnetworks/default",
                "subnetwork_project": "fluid-faha081j"
              }
            ],
            "project": "fluid-faha081j",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss1",
            "service_account": [
              {
                "email": "201229081417-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "lustre"
            ],
            "tags_fingerprint": "CgvKniLKZUs=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "google_compute_disk.ost"
          ]
        },
        {
          "index_key": 2,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": true,
            "attached_disk": [
              {
                "device_name": "ost",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost2"
              }
            ],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200618",
                    "labels": {},
                    "size": 20,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-oss2"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "deletion_protection": false,
            "description": "",
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss2",
            "instance_id": "2026927736380183638",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-8",
            "metadata": {
              "cluster-name": "lustre",
              "e2fs-version": "latest",
              "enable-oslogin": "TRUE",
              "fs-name": "lustre",
              "hsm-gcs": "",
              "hsm-gcs-prefix": "",
              "lustre-version": "latest-release",
              "node-role": "OSS"
            },
            "metadata_fingerprint": "8Buto1H12no=",
            "metadata_startup_script": "#!/bin/bash\n\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Imported variables from lustre.jinja, do not modify\nCLUSTER_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/cluster-name\" -H \"Metadata-Flavor: Google\")\nFS_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/fs-name\" -H \"Metadata-Flavor: Google\")\nNODE_ROLE=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/node-role\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET_IMPORT=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs-prefix\" -H \"Metadata-Flavor: Google\")\nLUSTRE_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/lustre-version\" -H \"Metadata-Flavor: Google\")\nE2FS_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/e2fs-version\" -H \"Metadata-Flavor: Google\")\n\nost_mount_point=\"/mnt/ost\"\nmdt_mount_point=\"/mnt/mdt\"\n\nMDS_HOSTNAME=\"${CLUSTER_NAME}-mds0\"\nLUSTRE_CLIENT_VERSION=\"lustre-2.10.8\"\nLUSTRE_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_VERSION}/el7/server/RPMS/x86_64/\"\nLUSTRE_CLIENT_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_CLIENT_VERSION}/el7/client/RPMS/x86_64/\"\nE2FS_URL=\"https://downloads.whamcloud.com/public/e2fsprogs/${E2FS_VERSION}/el7/RPMS/x86_64/\"\n\n# Array of all required RPMs for Lustre\nLUSTRE_RPMS=(\"kernel-3*.rpm\"\n\"kernel-devel-*.rpm\" \n\"kernel-debuginfo-common-*.rpm\"\n\"lustre-2*.rpm\"\n\"lustre-ldiskfs-dkms-*.rpm\"\n\"lustre-osd-ldiskfs-mount-*.rpm\"\n\"kmod-lustre-2*.rpm\"\n\"kmod-lustre-osd-ldiskfs-*.rpm\")\n\nLUSTRE_CLIENT_RPMS=(\"kmod-lustre-client-2*.rpm\"\n\"lustre-client-2*.rpm\")\n\n# Array of all required RPMs for E2FS\nE2FS_RPMS=(\"e2fsprogs-1*.rpm\"\n\"e2fsprogs-libs-1*.rpm\"\n\"libss-1*.rpm\"\n\"libcom_err-1*.rpm\")\n\n# Install updates and minimum packages to install Lustre\nfunction yum_install() {\n\tyum update -y\n\tyum install -y net-snmp-libs expect patch dkms gcc libyaml-devel mdadm epel-release pdsh\n}\n\n# Set Message of the Day declaring that Lustre is being installed\nfunction start_motd() {\n\tmotd=\"*** Lustre is currently being installed in the background. ***\n***  Please wait until notified the installation is done.  ***\"\n\techo \"$motd\" \u003e /etc/motd\n}\n\n#Set Message of the Day to show Lustre cluster information and declare the Lustre installation complete\nfunction end_motd() {\n\techo -e \"Welcome to the Google Cloud Lustre Deployment!\\nLustre MDS: $MDS_HOSTNAME\\nLustre FS Name: $FS_NAME\\nMount Command: mount -t lustre $MDS_HOSTNAME:/$FS_NAME \u003clocal dir\u003e\" \u003e /etc/motd\n\twall -n \"*** Lustre installation is complete! *** \"\n\twall -n \"`cat /etc/motd`\"\n}\n\n# Wait in a loop until the internet is up\nfunction wait_for_internet() {\n\ttime=0\n\t#Loop over one ping to google.com, and while no response\n\twhile [ `ping google.com -c1 | grep \"time=\" -c` -eq 0 ]; do\n\t\tsleep 5\n\t\tlet time+=1\n\t\t# If we try 60 times (300 seconds) without success, abort\n\t\tif [ $time -gt 60 ]; then\n\t\t\techo \"Internet has not connected. Aborting installation.\"\n\t\t\texit 1\n\t\tfi\n\tdone\n}\n\n# Delete RPMs after installation\nfunction cleanup() {\n\trm -rf /lustre/*.rpm\n}\n\n# Install Lemur Lustre HSM Data Mover\nfunction install_lemur() {\n\tyum install -y wget go make rpm-build libselinux-devel libyaml-devel zlib-devel\n\tyum groupinstall -y \"Development Tools\" --skip-broken\n\n\tmkdir /lustre\n\tcd /lustre\n\n\tgit clone https://github.com/mhugues/lemur.git\n\tcd lemur\n\n\tgit checkout feature/lhsm-plugin-gcs\n\n\tsudo make local-rpm\n\n\trpm -ivh /root/rpmbuild/RPMS/x86_64/*.rpm\n\n}\n\nfunction configure_lemur() {\n\t#Lemur Agent Configuration\n\tmkdir -p /var/lib/lhsmd/roots\n\n\tcat \u003e /etc/lhsmd/agent \u003c\u003c EOF\n## The mount target for the Lustre file system that will be used with this agent.\n##\nclient_device=  \"${MDS_HOSTNAME}@tcp:/lustre\"\n\n##\n## Base directory used for the Lustre mount points created by the agent\n##\nmount_root= \"/var/lib/lhsmd/roots\"\n\n##\n## List of enabled plugins\n##\nenabled_plugins = [\"lhsm-plugin-gcs\"]\n\n##\n## Directory to look for the plugins\n##\nplugin_dir = \"/usr/libexec/lhsmd\"\n\n##\n## Number of threads handling incoming HSM requests.\n##\nhandler_count = 8\n\n##\n## Enable experimental snapshot feature.\n##\nsnapshots {\n     enabled = false\n}\n\nEOF\n\n\t#Lemur GCS Plugin Conf\n\tcat \u003e /etc/lhsmd/lhsm-plugin-gcs \u003c\u003c EOF\n## Credentials file in Json format from the service account (Optional)\n#service_account_key=\"[SA_NAME-key.json]\"\n\n## Maximum number of concurrent copies.\n##\nnum_threads=8\n\n##\n## One or more archive definition is required.\n##\narchive \"1\" {\n        id = 1\n        bucket = \"${HSM_GCS_BUCKET:5}\"\n}\nEOF\n\n\tchmod 600 /etc/lhsmd/lhsm-plugin-gcs\n\n\t#Start lhsm agent daemon\n\tsystemctl start lhsmd\n\n\n}\n\nfunction hsm_import_bucket()\n{\n\n\tbucket_file_list=`gsutil ls -r ${HSM_GCS_BUCKET_IMPORT}/** | sed \"/\\/:$/d\"`\n\n\tfor i in $bucket_file_list\n\tdo\n\t\t# Convert to destination file full path\n\t\tdest_file_name=`echo ${i} | sed \"s%${HSM_GCS_BUCKET_IMPORT}%/mnt/%g\"`\n\t\tdir_name=`dirname ${dest_file_name}`\n\n\t\tif [ ! -d ${dir_name} ]; then\n\t\t\tmkdir -p ${dir_name}\n\t\tfi\n\n\t\tsrc_file_name=`echo $i | sed \"s%gs://.[^/]*/%%g\"`\n\t\tlhsm import --uuid ${src_file_name} --uid 0 --gid 0 ${dest_file_name}\n\n\tdone\n}\n\nfunction install_lustre_client() {\n\n\tyum install -y git\n\n\tgit clone git://git.whamcloud.com/fs/lustre-release.git \u0026\u0026 cd lustre-release\n\tgit checkout 2.10.8\n\n\tyum install -y kernel-devel kernel-headers kernel-debug libtool libyaml-devel libselinux-devel zlib-devel rpm-build\n\n\tsh ./autogen.sh\n\t./configure --disable-server --with-o2ib=no --enable-client\n\tmake rpms\n\n\tmkdir -p /lustre/lustre_client_release\n\tcp *.rpm /lustre/lustre_client_release/\n\trpm -ivh /lustre/lustre_client_release/${LUSTRE_CLIENT_RPMS[@]}\n\n}\n\nfunction main() {\n\t# Check for an install.log, if it doesn't exist begin install\n\tif [ ! -e /lustre/install.log ]; then\n\t\tstart_motd\n\t\twait_for_internet\n\t\t# Install wget\n\t\tyum install -y wget\n\n\t\tmkdir /lustre\n\t\tcd /lustre\n\n\t\tif [ \"$NODE_ROLE\" != \"HSM\" ]; then\n\t\t\t# Update and install packages in background while RPMs are downloaded\n\t\t\tyum_install \u0026\n\n\t\t\t# Loop over RPM arrays and download them locally\n\t\t\tfor i in ${LUSTRE_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${LUSTRE_URL} -P /lustre; done\n\t\t\tfor i in ${E2FS_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${E2FS_URL} -P /lustre; done\n\t\t\tfind /lustre -name \"*.rpm\" | xargs -I{} mv {} /lustre\n\t\t\n\t\t\t# Wait for yum_install for finish\n\t\t\twait `jobs -p`\n\t\tfi\n\t\t\n\t\t# Disable yum-crom, firewalld, and disable SELINUX (Lustre requirements)\n\t\tsystemctl disable yum-cron.service\n\t\tsystemctl stop yum-cron\n\t\tsed -i -e \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config\n\t\tsystemctl stop firewalld\n\t\tsystemctl disable firewalld\n\t\t\n\t\t# Install all downloaded RPMs\n\t\trpm -ivh --force *.rpm\n\t\tif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\tinstall_lustre_client\n\t\tfi\n\t\n\t\t# Mark install.log as reaching stage 1\n\t\techo 1 \u003e /lustre/install.log\n\t\t# Reboot to switch to the Lustre kernel\n\t\treboot\n\t# If the install.log exists, and contains a 1, begin installation stage 2\n\telif [ \"`cat /lustre/install.log`\" == \"1\" ]; then\n\t\t# Load the Lustre kernel modules\n\t\tmodprobe lustre\n\t\t\n\t\t# Get the hostname index (trailing digit on the hostname) \n\t\thost_index=`hostname | grep -o -e '[[:digit:]]*' | tail -1`\n\t\t# Decrement the index by 1 to convert to Lustre's indexing\n\t\tif [ ! $host_index ]; then\n\t\t\thost_index=0\n\t\telse\n\t\t\tlet host_index=host_index-1\n\t\tfi\n\n\t\t# Determine if the OST/MDT disk is PD or Local SSD\n        \tnum_local_ssds=`lsblk | grep -c nvme`\n        \tif [ ${num_local_ssds} -gt 1 ]; then\n\t        \tlustre_device=\"/dev/md0\"\n\t        \tsudo mdadm --create ${lustre_device} --level=0 --raid-devices=${num_local_ssds} /dev/nvme0n*\n        \telif [ $num_local_ssds -eq 1 ]; then\n\t        \tlustre_device=\"/dev/nvme0n1\"\n        \telse\n\t        \tlustre_device=\"/dev/sdb\"\n        \tfi\n\t\t\n\t\t# If the local node running this script is a Lustre MDS, install the MDS/MGS software\n\t\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response\n\t\t\twhile [ `sudo lctl ping ${CLUSTER_NAME}-oss0 | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\tmkdir $mdt_mount_point\n\t\t\tmkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$mdt_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\n\t\t\t# Enable HSM on the Lustre MGS\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm_control=enabled\n\n\t\t\t# Setup the archive id for the specific HSM backend, we are only using 1 so id=1 is just fine\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm.default_archive_id=1\n\n\t\t\t# Increase the number of HSM Max requests on the MDT, you may want to\n\t\t\t# experiment with various values if you intend to go to production\n\t\t\tlctl set_param mdt.*-MDT0000.hsm.max_requests=128\n\n\t\t\t# Once the network is up, make the lustre filesystem on the MDT\n\t\t\t#mkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} /dev/sdb\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\t#mkdir /mdt\n\t\t\t#mount -t lustre /dev/sdb /mdt\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $mdt_mount_point` -eq 0 ]; then\n\t\t\t\techo \"MDT mount has failed. Please try mounting manually with \"mount -t lustre $lustre_device $mdt_mount_point\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\n\t\t\t# Disable the authentication upcall by default, change if using auth\n\t\t\techo NONE \u003e /proc/fs/lustre/mdt/lustre-MDT0000/identity_upcall\n\t\t# If the local node running this script is a Lustre OSS, install the OSS software\n\t\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the Lustre OST\n\t\t\t# Sleep 60 seconds to give MDS/MGS time to come up before the OSS. More robust communication would be good.\n\t\t\tsleep 60\n\n\t\t\t# Make the directory to mount the OST, and mount the OST\n\t\t\tmkdir $ost_mount_point\n\t\t\tmkfs.lustre --ost --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$ost_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $ost_mount_point` -eq 0 ]; then\n\t\t\t\techo \"OST mount has failed. Please try mounting manually with \\\"mount -t lustre $lustre_device $ost_mount_point\\\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\t\t# If the local node running this script is a Lustre HSM Data Mover, install the Lemur software\n\t\telif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\tmount_status=1\n\n\t\t\twhile [ ${mount_status} -ne 0 ]; do\n\t\t\t\tmount -t lustre ${MDS_HOSTNAME}:/${FS_NAME} /mnt\n\t\t\t\tmount_status=$?\n\t\t\tdone\n\n\t\t\tinstall_lemur\n\t\t\tconfigure_lemur\n\n\t\t\tif [ ! -z \"${HSM_GCS_BUCKET_IMPORT}\" ]; then\n\t\t\t\thsm_import_bucket\n\t\t\tfi\n\n\t\tfi\n\t\t# Mark install.log as reaching stage 2\n\t\techo 2 \u003e /lustre/install.log\n\t\t# Change MOTD to mark install as complete\n\t\tend_motd\n\t\t# Run the cleanup function to remove RPMs\n\t\tcleanup\n\t# If install.log shows stage 2, then Lustre is installed and just needs to be started\n\t#else\n\t\t# If it's an MDS/MGS, mount the MDT\n\t#\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t#\t\tmount -t lustre /dev/sdb /mdt\n\t\t# If it's an OSS, sleep to let the MDT mount, and then mount the OST\n\t#\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t#\t\tsleep 20\n\t#\t\tmount -t lustre /dev/sdb $ost_mount_point\n\t#\tfi\n\tfi\n}\n\nmain $@\n",
            "min_cpu_platform": "",
            "name": "lustre-oss2",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "34.105.81.228",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/global/networks/default",
                "network_ip": "10.138.0.36",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/regions/us-west1/subnetworks/default",
                "subnetwork_project": "fluid-faha081j"
              }
            ],
            "project": "fluid-faha081j",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss2",
            "service_account": [
              {
                "email": "201229081417-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "lustre"
            ],
            "tags_fingerprint": "CgvKniLKZUs=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "google_compute_disk.ost"
          ]
        },
        {
          "index_key": 3,
          "schema_version": 6,
          "attributes": {
            "allow_stopping_for_update": true,
            "attached_disk": [
              {
                "device_name": "ost",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-ost3"
              }
            ],
            "boot_disk": [
              {
                "auto_delete": true,
                "device_name": "persistent-disk-0",
                "disk_encryption_key_raw": "",
                "disk_encryption_key_sha256": "",
                "initialize_params": [
                  {
                    "image": "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200618",
                    "labels": {},
                    "size": 20,
                    "type": "pd-standard"
                  }
                ],
                "kms_key_self_link": "",
                "mode": "READ_WRITE",
                "source": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/disks/lustre-oss3"
              }
            ],
            "can_ip_forward": false,
            "cpu_platform": "Intel Broadwell",
            "deletion_protection": false,
            "description": "",
            "enable_display": false,
            "guest_accelerator": [],
            "hostname": "",
            "id": "projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss3",
            "instance_id": "760346970326029398",
            "label_fingerprint": "42WmSpB8rSM=",
            "labels": null,
            "machine_type": "n1-standard-8",
            "metadata": {
              "cluster-name": "lustre",
              "e2fs-version": "latest",
              "enable-oslogin": "TRUE",
              "fs-name": "lustre",
              "hsm-gcs": "",
              "hsm-gcs-prefix": "",
              "lustre-version": "latest-release",
              "node-role": "OSS"
            },
            "metadata_fingerprint": "8Buto1H12no=",
            "metadata_startup_script": "#!/bin/bash\n\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Imported variables from lustre.jinja, do not modify\nCLUSTER_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/cluster-name\" -H \"Metadata-Flavor: Google\")\nFS_NAME=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/fs-name\" -H \"Metadata-Flavor: Google\")\nNODE_ROLE=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/node-role\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs\" -H \"Metadata-Flavor: Google\")\nHSM_GCS_BUCKET_IMPORT=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/hsm-gcs-prefix\" -H \"Metadata-Flavor: Google\")\nLUSTRE_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/lustre-version\" -H \"Metadata-Flavor: Google\")\nE2FS_VERSION=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/e2fs-version\" -H \"Metadata-Flavor: Google\")\n\nost_mount_point=\"/mnt/ost\"\nmdt_mount_point=\"/mnt/mdt\"\n\nMDS_HOSTNAME=\"${CLUSTER_NAME}-mds0\"\nLUSTRE_CLIENT_VERSION=\"lustre-2.10.8\"\nLUSTRE_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_VERSION}/el7/server/RPMS/x86_64/\"\nLUSTRE_CLIENT_URL=\"https://downloads.whamcloud.com/public/lustre/${LUSTRE_CLIENT_VERSION}/el7/client/RPMS/x86_64/\"\nE2FS_URL=\"https://downloads.whamcloud.com/public/e2fsprogs/${E2FS_VERSION}/el7/RPMS/x86_64/\"\n\n# Array of all required RPMs for Lustre\nLUSTRE_RPMS=(\"kernel-3*.rpm\"\n\"kernel-devel-*.rpm\" \n\"kernel-debuginfo-common-*.rpm\"\n\"lustre-2*.rpm\"\n\"lustre-ldiskfs-dkms-*.rpm\"\n\"lustre-osd-ldiskfs-mount-*.rpm\"\n\"kmod-lustre-2*.rpm\"\n\"kmod-lustre-osd-ldiskfs-*.rpm\")\n\nLUSTRE_CLIENT_RPMS=(\"kmod-lustre-client-2*.rpm\"\n\"lustre-client-2*.rpm\")\n\n# Array of all required RPMs for E2FS\nE2FS_RPMS=(\"e2fsprogs-1*.rpm\"\n\"e2fsprogs-libs-1*.rpm\"\n\"libss-1*.rpm\"\n\"libcom_err-1*.rpm\")\n\n# Install updates and minimum packages to install Lustre\nfunction yum_install() {\n\tyum update -y\n\tyum install -y net-snmp-libs expect patch dkms gcc libyaml-devel mdadm epel-release pdsh\n}\n\n# Set Message of the Day declaring that Lustre is being installed\nfunction start_motd() {\n\tmotd=\"*** Lustre is currently being installed in the background. ***\n***  Please wait until notified the installation is done.  ***\"\n\techo \"$motd\" \u003e /etc/motd\n}\n\n#Set Message of the Day to show Lustre cluster information and declare the Lustre installation complete\nfunction end_motd() {\n\techo -e \"Welcome to the Google Cloud Lustre Deployment!\\nLustre MDS: $MDS_HOSTNAME\\nLustre FS Name: $FS_NAME\\nMount Command: mount -t lustre $MDS_HOSTNAME:/$FS_NAME \u003clocal dir\u003e\" \u003e /etc/motd\n\twall -n \"*** Lustre installation is complete! *** \"\n\twall -n \"`cat /etc/motd`\"\n}\n\n# Wait in a loop until the internet is up\nfunction wait_for_internet() {\n\ttime=0\n\t#Loop over one ping to google.com, and while no response\n\twhile [ `ping google.com -c1 | grep \"time=\" -c` -eq 0 ]; do\n\t\tsleep 5\n\t\tlet time+=1\n\t\t# If we try 60 times (300 seconds) without success, abort\n\t\tif [ $time -gt 60 ]; then\n\t\t\techo \"Internet has not connected. Aborting installation.\"\n\t\t\texit 1\n\t\tfi\n\tdone\n}\n\n# Delete RPMs after installation\nfunction cleanup() {\n\trm -rf /lustre/*.rpm\n}\n\n# Install Lemur Lustre HSM Data Mover\nfunction install_lemur() {\n\tyum install -y wget go make rpm-build libselinux-devel libyaml-devel zlib-devel\n\tyum groupinstall -y \"Development Tools\" --skip-broken\n\n\tmkdir /lustre\n\tcd /lustre\n\n\tgit clone https://github.com/mhugues/lemur.git\n\tcd lemur\n\n\tgit checkout feature/lhsm-plugin-gcs\n\n\tsudo make local-rpm\n\n\trpm -ivh /root/rpmbuild/RPMS/x86_64/*.rpm\n\n}\n\nfunction configure_lemur() {\n\t#Lemur Agent Configuration\n\tmkdir -p /var/lib/lhsmd/roots\n\n\tcat \u003e /etc/lhsmd/agent \u003c\u003c EOF\n## The mount target for the Lustre file system that will be used with this agent.\n##\nclient_device=  \"${MDS_HOSTNAME}@tcp:/lustre\"\n\n##\n## Base directory used for the Lustre mount points created by the agent\n##\nmount_root= \"/var/lib/lhsmd/roots\"\n\n##\n## List of enabled plugins\n##\nenabled_plugins = [\"lhsm-plugin-gcs\"]\n\n##\n## Directory to look for the plugins\n##\nplugin_dir = \"/usr/libexec/lhsmd\"\n\n##\n## Number of threads handling incoming HSM requests.\n##\nhandler_count = 8\n\n##\n## Enable experimental snapshot feature.\n##\nsnapshots {\n     enabled = false\n}\n\nEOF\n\n\t#Lemur GCS Plugin Conf\n\tcat \u003e /etc/lhsmd/lhsm-plugin-gcs \u003c\u003c EOF\n## Credentials file in Json format from the service account (Optional)\n#service_account_key=\"[SA_NAME-key.json]\"\n\n## Maximum number of concurrent copies.\n##\nnum_threads=8\n\n##\n## One or more archive definition is required.\n##\narchive \"1\" {\n        id = 1\n        bucket = \"${HSM_GCS_BUCKET:5}\"\n}\nEOF\n\n\tchmod 600 /etc/lhsmd/lhsm-plugin-gcs\n\n\t#Start lhsm agent daemon\n\tsystemctl start lhsmd\n\n\n}\n\nfunction hsm_import_bucket()\n{\n\n\tbucket_file_list=`gsutil ls -r ${HSM_GCS_BUCKET_IMPORT}/** | sed \"/\\/:$/d\"`\n\n\tfor i in $bucket_file_list\n\tdo\n\t\t# Convert to destination file full path\n\t\tdest_file_name=`echo ${i} | sed \"s%${HSM_GCS_BUCKET_IMPORT}%/mnt/%g\"`\n\t\tdir_name=`dirname ${dest_file_name}`\n\n\t\tif [ ! -d ${dir_name} ]; then\n\t\t\tmkdir -p ${dir_name}\n\t\tfi\n\n\t\tsrc_file_name=`echo $i | sed \"s%gs://.[^/]*/%%g\"`\n\t\tlhsm import --uuid ${src_file_name} --uid 0 --gid 0 ${dest_file_name}\n\n\tdone\n}\n\nfunction install_lustre_client() {\n\n\tyum install -y git\n\n\tgit clone git://git.whamcloud.com/fs/lustre-release.git \u0026\u0026 cd lustre-release\n\tgit checkout 2.10.8\n\n\tyum install -y kernel-devel kernel-headers kernel-debug libtool libyaml-devel libselinux-devel zlib-devel rpm-build\n\n\tsh ./autogen.sh\n\t./configure --disable-server --with-o2ib=no --enable-client\n\tmake rpms\n\n\tmkdir -p /lustre/lustre_client_release\n\tcp *.rpm /lustre/lustre_client_release/\n\trpm -ivh /lustre/lustre_client_release/${LUSTRE_CLIENT_RPMS[@]}\n\n}\n\nfunction main() {\n\t# Check for an install.log, if it doesn't exist begin install\n\tif [ ! -e /lustre/install.log ]; then\n\t\tstart_motd\n\t\twait_for_internet\n\t\t# Install wget\n\t\tyum install -y wget\n\n\t\tmkdir /lustre\n\t\tcd /lustre\n\n\t\tif [ \"$NODE_ROLE\" != \"HSM\" ]; then\n\t\t\t# Update and install packages in background while RPMs are downloaded\n\t\t\tyum_install \u0026\n\n\t\t\t# Loop over RPM arrays and download them locally\n\t\t\tfor i in ${LUSTRE_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${LUSTRE_URL} -P /lustre; done\n\t\t\tfor i in ${E2FS_RPMS[@]}; do wget -r -l1 --no-parent -A \"$i\" ${E2FS_URL} -P /lustre; done\n\t\t\tfind /lustre -name \"*.rpm\" | xargs -I{} mv {} /lustre\n\t\t\n\t\t\t# Wait for yum_install for finish\n\t\t\twait `jobs -p`\n\t\tfi\n\t\t\n\t\t# Disable yum-crom, firewalld, and disable SELINUX (Lustre requirements)\n\t\tsystemctl disable yum-cron.service\n\t\tsystemctl stop yum-cron\n\t\tsed -i -e \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config\n\t\tsystemctl stop firewalld\n\t\tsystemctl disable firewalld\n\t\t\n\t\t# Install all downloaded RPMs\n\t\trpm -ivh --force *.rpm\n\t\tif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\tinstall_lustre_client\n\t\tfi\n\t\n\t\t# Mark install.log as reaching stage 1\n\t\techo 1 \u003e /lustre/install.log\n\t\t# Reboot to switch to the Lustre kernel\n\t\treboot\n\t# If the install.log exists, and contains a 1, begin installation stage 2\n\telif [ \"`cat /lustre/install.log`\" == \"1\" ]; then\n\t\t# Load the Lustre kernel modules\n\t\tmodprobe lustre\n\t\t\n\t\t# Get the hostname index (trailing digit on the hostname) \n\t\thost_index=`hostname | grep -o -e '[[:digit:]]*' | tail -1`\n\t\t# Decrement the index by 1 to convert to Lustre's indexing\n\t\tif [ ! $host_index ]; then\n\t\t\thost_index=0\n\t\telse\n\t\t\tlet host_index=host_index-1\n\t\tfi\n\n\t\t# Determine if the OST/MDT disk is PD or Local SSD\n        \tnum_local_ssds=`lsblk | grep -c nvme`\n        \tif [ ${num_local_ssds} -gt 1 ]; then\n\t        \tlustre_device=\"/dev/md0\"\n\t        \tsudo mdadm --create ${lustre_device} --level=0 --raid-devices=${num_local_ssds} /dev/nvme0n*\n        \telif [ $num_local_ssds -eq 1 ]; then\n\t        \tlustre_device=\"/dev/nvme0n1\"\n        \telse\n\t        \tlustre_device=\"/dev/sdb\"\n        \tfi\n\t\t\n\t\t# If the local node running this script is a Lustre MDS, install the MDS/MGS software\n\t\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response\n\t\t\twhile [ `sudo lctl ping ${CLUSTER_NAME}-oss0 | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\tmkdir $mdt_mount_point\n\t\t\tmkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$mdt_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\n\t\t\t# Enable HSM on the Lustre MGS\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm_control=enabled\n\n\t\t\t# Setup the archive id for the specific HSM backend, we are only using 1 so id=1 is just fine\n\t\t\tlctl set_param -P mdt.*-MDT0000.hsm.default_archive_id=1\n\n\t\t\t# Increase the number of HSM Max requests on the MDT, you may want to\n\t\t\t# experiment with various values if you intend to go to production\n\t\t\tlctl set_param mdt.*-MDT0000.hsm.max_requests=128\n\n\t\t\t# Once the network is up, make the lustre filesystem on the MDT\n\t\t\t#mkfs.lustre --mdt --mgs --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} /dev/sdb\n\n\t\t\t# Make the MDT mount and mount the device\n\t\t\t#mkdir /mdt\n\t\t\t#mount -t lustre /dev/sdb /mdt\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $mdt_mount_point` -eq 0 ]; then\n\t\t\t\techo \"MDT mount has failed. Please try mounting manually with \"mount -t lustre $lustre_device $mdt_mount_point\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\n\t\t\t# Disable the authentication upcall by default, change if using auth\n\t\t\techo NONE \u003e /proc/fs/lustre/mdt/lustre-MDT0000/identity_upcall\n\t\t# If the local node running this script is a Lustre OSS, install the OSS software\n\t\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\t# Make the Lustre OST\n\t\t\t# Sleep 60 seconds to give MDS/MGS time to come up before the OSS. More robust communication would be good.\n\t\t\tsleep 60\n\n\t\t\t# Make the directory to mount the OST, and mount the OST\n\t\t\tmkdir $ost_mount_point\n\t\t\tmkfs.lustre --ost --index=${host_index} --fsname=${FS_NAME} --mgsnode=${MDS_HOSTNAME} $lustre_device\n\t\t\techo \"$lustre_device\t$ost_mount_point\tlustre\" \u003e\u003e /etc/fstab\n\t\t\tmount -a\n\t\t\t\n\t\t\t# Check for a successful mount, and fail otherwise.\n\t\t\tif [ `mount | grep -c $ost_mount_point` -eq 0 ]; then\n\t\t\t\techo \"OST mount has failed. Please try mounting manually with \\\"mount -t lustre $lustre_device $ost_mount_point\\\", or reboot this node.\"\n\t\t\t\texit 1\n\t\t\tfi\n\t\t# If the local node running this script is a Lustre HSM Data Mover, install the Lemur software\n\t\telif [ \"$NODE_ROLE\" == \"HSM\" ]; then\n\t\t\t# Do LCTL ping to the OSS nodes and sleep until LNET is up and we get a response \n\t\t\twhile [ `sudo lctl ping ${MDS_HOSTNAME} | grep -c \"Input/output error\"` -gt 0 ]; do\n\t\t\t\tsleep 10\n\t\t\tdone\n\n\t\t\tmount_status=1\n\n\t\t\twhile [ ${mount_status} -ne 0 ]; do\n\t\t\t\tmount -t lustre ${MDS_HOSTNAME}:/${FS_NAME} /mnt\n\t\t\t\tmount_status=$?\n\t\t\tdone\n\n\t\t\tinstall_lemur\n\t\t\tconfigure_lemur\n\n\t\t\tif [ ! -z \"${HSM_GCS_BUCKET_IMPORT}\" ]; then\n\t\t\t\thsm_import_bucket\n\t\t\tfi\n\n\t\tfi\n\t\t# Mark install.log as reaching stage 2\n\t\techo 2 \u003e /lustre/install.log\n\t\t# Change MOTD to mark install as complete\n\t\tend_motd\n\t\t# Run the cleanup function to remove RPMs\n\t\tcleanup\n\t# If install.log shows stage 2, then Lustre is installed and just needs to be started\n\t#else\n\t\t# If it's an MDS/MGS, mount the MDT\n\t#\tif [ \"$NODE_ROLE\" == \"MDS\" ]; then\n\t#\t\tmount -t lustre /dev/sdb /mdt\n\t\t# If it's an OSS, sleep to let the MDT mount, and then mount the OST\n\t#\telif [ \"$NODE_ROLE\" == \"OSS\" ]; then\n\t#\t\tsleep 20\n\t#\t\tmount -t lustre /dev/sdb $ost_mount_point\n\t#\tfi\n\tfi\n}\n\nmain $@\n",
            "min_cpu_platform": "",
            "name": "lustre-oss3",
            "network_interface": [
              {
                "access_config": [
                  {
                    "nat_ip": "104.198.99.218",
                    "network_tier": "PREMIUM",
                    "public_ptr_domain_name": ""
                  }
                ],
                "alias_ip_range": [],
                "name": "nic0",
                "network": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/global/networks/default",
                "network_ip": "10.138.0.37",
                "subnetwork": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/regions/us-west1/subnetworks/default",
                "subnetwork_project": "fluid-faha081j"
              }
            ],
            "project": "fluid-faha081j",
            "scheduling": [
              {
                "automatic_restart": true,
                "node_affinities": [],
                "on_host_maintenance": "MIGRATE",
                "preemptible": false
              }
            ],
            "scratch_disk": [],
            "self_link": "https://www.googleapis.com/compute/v1/projects/fluid-faha081j/zones/us-west1-a/instances/lustre-oss3",
            "service_account": [
              {
                "email": "201229081417-compute@developer.gserviceaccount.com",
                "scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ]
              }
            ],
            "shielded_instance_config": [
              {
                "enable_integrity_monitoring": true,
                "enable_secure_boot": false,
                "enable_vtpm": true
              }
            ],
            "tags": [
              "lustre"
            ],
            "tags_fingerprint": "CgvKniLKZUs=",
            "timeouts": null,
            "zone": "us-west1-a"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiNiJ9",
          "dependencies": [
            "google_compute_disk.ost"
          ]
        }
      ]
    }
  ]
}
